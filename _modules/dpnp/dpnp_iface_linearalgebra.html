

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" />
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-554F8VNE28"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-554F8VNE28');
    </script>
    
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>dpnp.dpnp_iface_linearalgebra &mdash; Data Parallel Extension for NumPy 0.20.0dev1+45.g6ff93b98f15 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=6ad02682"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Data Parallel Extension for NumPy
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start_guide.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/index.html">API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dpnp_backend_api.html">C++ backend API Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Data Parallel Extension for NumPy</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">dpnp.dpnp_iface_linearalgebra</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for dpnp.dpnp_iface_linearalgebra</h1><div class="highlight"><pre>
<span></span><span class="c1"># *****************************************************************************</span>
<span class="c1"># Copyright (c) 2016, Intel Corporation</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Redistribution and use in source and binary forms, with or without</span>
<span class="c1"># modification, are permitted provided that the following conditions are met:</span>
<span class="c1"># - Redistributions of source code must retain the above copyright notice,</span>
<span class="c1">#   this list of conditions and the following disclaimer.</span>
<span class="c1"># - Redistributions in binary form must reproduce the above copyright notice,</span>
<span class="c1">#   this list of conditions and the following disclaimer in the documentation</span>
<span class="c1">#   and/or other materials provided with the distribution.</span>
<span class="c1"># - Neither the name of the copyright holder nor the names of its contributors</span>
<span class="c1">#   may be used to endorse or promote products derived from this software</span>
<span class="c1">#   without specific prior written permission.</span>
<span class="c1">#</span>
<span class="c1"># THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS &quot;AS IS&quot;</span>
<span class="c1"># AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE</span>
<span class="c1"># IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE</span>
<span class="c1"># ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE</span>
<span class="c1"># LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR</span>
<span class="c1"># CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF</span>
<span class="c1"># SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS</span>
<span class="c1"># INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN</span>
<span class="c1"># CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)</span>
<span class="c1"># ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF</span>
<span class="c1"># THE POSSIBILITY OF SUCH DAMAGE.</span>
<span class="c1"># *****************************************************************************</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Interface of the Linear Algebra part of the DPNP</span>

<span class="sd">Notes</span>
<span class="sd">-----</span>
<span class="sd">This module is a face or public interface file for the library</span>
<span class="sd">it contains:</span>
<span class="sd"> - Interface functions</span>
<span class="sd"> - documentation for the functions</span>
<span class="sd"> - The functions parameters check</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># pylint: disable=no-name-in-module</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">dpnp</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.dpnp_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">map_dtype_to_device</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.dpnp_utils.dpnp_utils_einsum</span><span class="w"> </span><span class="kn">import</span> <span class="n">dpnp_einsum</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.dpnp_utils.dpnp_utils_linearalgebra</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">dpnp_dot</span><span class="p">,</span>
    <span class="n">dpnp_kron</span><span class="p">,</span>
    <span class="n">dpnp_multiplication</span><span class="p">,</span>
    <span class="n">dpnp_tensordot</span><span class="p">,</span>
    <span class="n">dpnp_vecdot</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1"># TODO: implement a specific scalar-array kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_call_multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">outer_calc</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adjusted multiply function for handling special cases of scalar-array dot</span>
<span class="sd">    products in linear algebra.</span>

<span class="sd">    `dpnp.multiply` cannot directly be used for calculating scalar-array dots,</span>
<span class="sd">    because the output dtype of multiply is not the same as the expected dtype</span>
<span class="sd">    for scalar-array dots. For example, if `sc` is an scalar and `a` is an</span>
<span class="sd">    array of type `float32`, then `dpnp.multiply(a, sc).dtype == dpnp.float32`</span>
<span class="sd">    (similar to NumPy). However, for scalar-array dots, such as the dot</span>
<span class="sd">    function, we need `dpnp.dot(a, sc).dtype == dpnp.float64` to align with</span>
<span class="sd">    NumPy. This functions adjusts the behavior of `dpnp.multiply` function to</span>
<span class="sd">    meet this requirement.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sc</span><span class="p">,</span> <span class="n">arr</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="k">if</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
    <span class="n">sc_dtype</span> <span class="o">=</span> <span class="n">map_dtype_to_device</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">sc</span><span class="p">),</span> <span class="n">arr</span><span class="o">.</span><span class="n">sycl_device</span><span class="p">)</span>
    <span class="n">res_dtype</span> <span class="o">=</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">result_type</span><span class="p">(</span><span class="n">sc_dtype</span><span class="p">,</span> <span class="n">arr</span><span class="p">)</span>
    <span class="n">multiply_func</span> <span class="o">=</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">multiply</span><span class="o">.</span><span class="n">outer</span> <span class="k">if</span> <span class="n">outer_calc</span> <span class="k">else</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">multiply</span>
    <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">out</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">arr</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">multiply_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">multiply_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">res_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">get_result_array</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">casting</span><span class="o">=</span><span class="s2">&quot;no&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="dot">
<a class="viewcode-back" href="../../reference/generated/dpnp.dot.html#dpnp.dot">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dot product of `a` and `b`.</span>

<span class="sd">    For full documentation refer to :obj:`numpy.dot`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : {dpnp.ndarray, usm_ndarray, scalar}</span>
<span class="sd">        First input array. Both inputs `a` and `b` can not be scalars</span>
<span class="sd">        at the same time.</span>
<span class="sd">    b : {dpnp.ndarray, usm_ndarray, scalar}</span>
<span class="sd">        Second input array. Both inputs `a` and `b` can not be scalars</span>
<span class="sd">        at the same time.</span>
<span class="sd">    out : {None, dpnp.ndarray, usm_ndarray}, optional</span>
<span class="sd">        Alternative output array in which to place the result. It must have</span>
<span class="sd">        the same shape and data type as the expected output and should be</span>
<span class="sd">        C-contiguous. If these conditions are not met, an exception is</span>
<span class="sd">        raised, instead of attempting to be flexible.</span>

<span class="sd">        Default: ``None``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : dpnp.ndarray</span>
<span class="sd">        Returns the dot product of `a` and `b`.</span>
<span class="sd">        If `out` is given, then it is returned.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    :obj:`dpnp.ndarray.dot` : Equivalent method.</span>
<span class="sd">    :obj:`dpnp.tensordot` : Sum products over arbitrary axes.</span>
<span class="sd">    :obj:`dpnp.vdot` : Complex-conjugating dot product.</span>
<span class="sd">    :obj:`dpnp.einsum` : Einstein summation convention.</span>
<span class="sd">    :obj:`dpnp.matmul` : Matrix product of two arrays.</span>
<span class="sd">    :obj:`dpnp.linalg.multi_dot` : Chained dot product.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import dpnp as np</span>
<span class="sd">    &gt;&gt;&gt; a = np.array([1, 2, 3])</span>
<span class="sd">    &gt;&gt;&gt; b = np.array([1, 2, 3])</span>
<span class="sd">    &gt;&gt;&gt; np.dot(a, b)</span>
<span class="sd">    array(14)</span>

<span class="sd">    Neither argument is complex-conjugated:</span>

<span class="sd">    &gt;&gt;&gt; np.dot(np.array([2j, 3j]), np.array([2j, 3j]))</span>
<span class="sd">    array(-13+0j)</span>

<span class="sd">    For 2-D arrays it is the matrix product:</span>

<span class="sd">    &gt;&gt;&gt; a = np.array([[1, 0], [0, 1]])</span>
<span class="sd">    &gt;&gt;&gt; b = np.array([[4, 1], [2, 2]])</span>
<span class="sd">    &gt;&gt;&gt; np.dot(a, b)</span>
<span class="sd">    array([[4, 1],</span>
<span class="sd">           [2, 2]])</span>

<span class="sd">    &gt;&gt;&gt; a = np.arange(3 * 4 * 5 * 6).reshape((3, 4, 5, 6))</span>
<span class="sd">    &gt;&gt;&gt; b = np.arange(3 * 4 * 5 * 6)[::-1].reshape((5, 4, 6, 3))</span>
<span class="sd">    &gt;&gt;&gt; np.dot(a, b)[2, 3, 2, 1, 2, 2]</span>
<span class="sd">    array(499128)</span>
<span class="sd">    &gt;&gt;&gt; sum(a[2, 3, 2, :] * b[1, 2, :, 2])</span>
<span class="sd">    array(499128)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dpnp</span><span class="o">.</span><span class="n">check_supported_arrays_type</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">scalar_type</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dpnp</span><span class="o">.</span><span class="n">check_supported_arrays_type</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">out</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">c_contiguous</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only C-contiguous array is acceptable.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">or</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_call_multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>

    <span class="n">a_ndim</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">b_ndim</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">ndim</span>
    <span class="k">if</span> <span class="n">a_ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">b_ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># TODO: use specific scalar-vector kernel</span>
        <span class="k">return</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>

    <span class="c1"># numpy.dot does not allow casting even if it is safe</span>
    <span class="c1"># casting=&quot;no&quot; is used in the following</span>
    <span class="k">if</span> <span class="n">a_ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">b_ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dpnp_dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span> <span class="n">casting</span><span class="o">=</span><span class="s2">&quot;no&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">a_ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">b_ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span> <span class="n">casting</span><span class="o">=</span><span class="s2">&quot;no&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">a_ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">b_ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span> <span class="n">casting</span><span class="o">=</span><span class="s2">&quot;no&quot;</span><span class="p">)</span>

    <span class="c1"># TODO: investigate usage of matmul for some possible</span>
    <span class="c1"># use cases instead of dpnp.tensordot</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">get_result_array</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">casting</span><span class="o">=</span><span class="s2">&quot;no&quot;</span><span class="p">)</span></div>



<div class="viewcode-block" id="einsum">
<a class="viewcode-back" href="../../reference/generated/dpnp.einsum.html#dpnp.einsum">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">einsum</span><span class="p">(</span>
    <span class="o">*</span><span class="n">operands</span><span class="p">,</span>
    <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">order</span><span class="o">=</span><span class="s2">&quot;K&quot;</span><span class="p">,</span>
    <span class="n">casting</span><span class="o">=</span><span class="s2">&quot;same_kind&quot;</span><span class="p">,</span>
    <span class="n">optimize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    einsum(subscripts, *operands, out=None, dtype=None, order=&quot;K&quot;, \</span>
<span class="sd">        casting=&quot;same_kind&quot;, optimize=False)</span>

<span class="sd">    Evaluates the Einstein summation convention on the operands.</span>

<span class="sd">    For full documentation refer to :obj:`numpy.einsum`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    subscripts : str</span>
<span class="sd">        Specifies the subscripts for summation as comma separated list of</span>
<span class="sd">        subscript labels. An implicit (classical Einstein summation)</span>
<span class="sd">        calculation is performed unless the explicit indicator &#39;-&gt;&#39; is</span>
<span class="sd">        included as well as subscript labels of the precise output form.</span>
<span class="sd">    *operands : sequence of {dpnp.ndarrays, usm_ndarray}</span>
<span class="sd">        These are the arrays for the operation.</span>
<span class="sd">    out : {dpnp.ndarrays, usm_ndarray, None}, optional</span>
<span class="sd">        If provided, the calculation is done into this array.</span>

<span class="sd">        Default: ``None``.</span>
<span class="sd">    dtype : {None, str, dtype object}, optional</span>
<span class="sd">        If provided, forces the calculation to use the data type specified.</span>

<span class="sd">        Default: ``None``.</span>
<span class="sd">    order : {None, &quot;C&quot;, &quot;F&quot;, &quot;A&quot;, &quot;K&quot;}, optional</span>
<span class="sd">        Controls the memory layout of the output. ``&quot;C&quot;`` means it should be</span>
<span class="sd">        C-contiguous. ``&quot;F&quot;`` means it should be F-contiguous, ``&quot;A&quot;`` means</span>
<span class="sd">        it should be ``&quot;F&quot;`` if the inputs are all ``&quot;F&quot;``, ``&quot;C&quot;`` otherwise.</span>
<span class="sd">        ``&quot;K&quot;`` means it should be as close to the layout as the inputs as</span>
<span class="sd">        is possible, including arbitrarily permuted axes. ``order=None`` is</span>
<span class="sd">        equivalent to ``order=&quot;K&quot;``.</span>

<span class="sd">        Default: ``&quot;K&quot;``.</span>
<span class="sd">    casting : {&quot;no&quot;, &quot;equiv&quot;, &quot;safe&quot;, &quot;same_kind&quot;, &quot;unsafe&quot;}, optional</span>
<span class="sd">        Controls what kind of data casting may occur. Setting this to</span>
<span class="sd">        ``&quot;unsafe&quot;`` is not recommended, as it can adversely affect</span>
<span class="sd">        accumulations.</span>

<span class="sd">          * ``&quot;no&quot;`` means the data types should not be cast at all.</span>
<span class="sd">          * ``&quot;equiv&quot;`` means only byte-order changes are allowed.</span>
<span class="sd">          * ``&quot;safe&quot;`` means only casts which can preserve values are allowed.</span>
<span class="sd">          * ``&quot;same_kind&quot;`` means only safe casts or casts within a kind,</span>
<span class="sd">            like float64 to float32, are allowed.</span>
<span class="sd">          * ``&quot;unsafe&quot;`` means any data conversions may be done.</span>

<span class="sd">        Please note that, in contrast to NumPy, the default setting here is</span>
<span class="sd">        ``&quot;same_kind&quot;``. This is to prevent errors that may occur when data</span>
<span class="sd">        needs to be converted to `float64`, but the device does not support it.</span>
<span class="sd">        In such cases, the data is instead converted to `float32`.</span>

<span class="sd">        Default: ``&quot;same_kind&quot;``.</span>
<span class="sd">    optimize : {False, True, &quot;greedy&quot;, &quot;optimal&quot;}, optional</span>
<span class="sd">        Controls if intermediate optimization should occur. No optimization</span>
<span class="sd">        will occur if ``False`` and ``True`` will default to the ``&quot;greedy&quot;``</span>
<span class="sd">        algorithm. Also accepts an explicit contraction list from the</span>
<span class="sd">        :obj:`dpnp.einsum_path` function.</span>

<span class="sd">        Default: ``False``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : dpnp.ndarray</span>
<span class="sd">        The calculation based on the Einstein summation convention.</span>

<span class="sd">    See Also</span>
<span class="sd">    -------</span>
<span class="sd">    :obj:`dpnp.einsum_path` : Evaluates the lowest cost contraction order</span>
<span class="sd">                              for an einsum expression.</span>
<span class="sd">    :obj:`dpnp.dot` : Returns the dot product of two arrays.</span>
<span class="sd">    :obj:`dpnp.inner` : Returns the inner product of two arrays.</span>
<span class="sd">    :obj:`dpnp.outer` : Returns the outer product of two arrays.</span>
<span class="sd">    :obj:`dpnp.tensordot` :  Sum products over arbitrary axes.</span>
<span class="sd">    :obj:`dpnp.linalg.multi_dot` : Chained dot product.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import dpnp as np</span>
<span class="sd">    &gt;&gt;&gt; a = np.arange(25).reshape(5, 5)</span>
<span class="sd">    &gt;&gt;&gt; b = np.arange(5)</span>
<span class="sd">    &gt;&gt;&gt; c = np.arange(6).reshape(2, 3)</span>

<span class="sd">    Trace of a matrix:</span>

<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;ii&quot;, a)</span>
<span class="sd">    array(60)</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(a, [0, 0])</span>
<span class="sd">    array(60)</span>
<span class="sd">    &gt;&gt;&gt; np.trace(a)</span>
<span class="sd">    array(60)</span>

<span class="sd">    Extract the diagonal (requires explicit form):</span>

<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;ii-&gt;i&quot;, a)</span>
<span class="sd">    array([ 0,  6, 12, 18, 24])</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(a, [0, 0], [0])</span>
<span class="sd">    array([ 0,  6, 12, 18, 24])</span>
<span class="sd">    &gt;&gt;&gt; np.diag(a)</span>
<span class="sd">    array([ 0,  6, 12, 18, 24])</span>

<span class="sd">    Sum over an axis (requires explicit form):</span>

<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;ij-&gt;i&quot;, a)</span>
<span class="sd">    array([ 10,  35,  60,  85, 110])</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(a, [0, 1], [0])</span>
<span class="sd">    array([ 10,  35,  60,  85, 110])</span>
<span class="sd">    &gt;&gt;&gt; np.sum(a, axis=1)</span>
<span class="sd">    array([ 10,  35,  60,  85, 110])</span>

<span class="sd">    For higher dimensional arrays summing a single axis can be done</span>
<span class="sd">    with ellipsis:</span>

<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;...j-&gt;...&quot;, a)</span>
<span class="sd">    array([ 10,  35,  60,  85, 110])</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(a, [Ellipsis,1], [Ellipsis])</span>
<span class="sd">    array([ 10,  35,  60,  85, 110])</span>

<span class="sd">    Compute a matrix transpose, or reorder any number of axes:</span>

<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;ji&quot;, c)</span>
<span class="sd">    array([[0, 3],</span>
<span class="sd">           [1, 4],</span>
<span class="sd">           [2, 5]])</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;ij-&gt;ji&quot;, c)</span>
<span class="sd">    array([[0, 3],</span>
<span class="sd">           [1, 4],</span>
<span class="sd">           [2, 5]])</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(c, [1, 0])</span>
<span class="sd">    array([[0, 3],</span>
<span class="sd">           [1, 4],</span>
<span class="sd">           [2, 5]])</span>
<span class="sd">    &gt;&gt;&gt; np.transpose(c)</span>
<span class="sd">    array([[0, 3],</span>
<span class="sd">           [1, 4],</span>
<span class="sd">           [2, 5]])</span>

<span class="sd">    Vector inner products:</span>

<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;i,i&quot;, b, b)</span>
<span class="sd">    array(30)</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(b, [0], b, [0])</span>
<span class="sd">    array(30)</span>
<span class="sd">    &gt;&gt;&gt; np.inner(b, b)</span>
<span class="sd">    array(30)</span>

<span class="sd">    Matrix vector multiplication:</span>

<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;ij,j&quot;, a, b)</span>
<span class="sd">    array([ 30,  80, 130, 180, 230])</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(a, [0, 1], b, [1])</span>
<span class="sd">    array([ 30,  80, 130, 180, 230])</span>
<span class="sd">    &gt;&gt;&gt; np.dot(a, b)</span>
<span class="sd">    array([ 30,  80, 130, 180, 230])</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;...j,j&quot;, a, b)</span>
<span class="sd">    array([ 30,  80, 130, 180, 230])</span>

<span class="sd">    Broadcasting and scalar multiplication:</span>

<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;..., ...&quot;, 3, c)</span>
<span class="sd">    array([[ 0,  3,  6],</span>
<span class="sd">           [ 9, 12, 15]])</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;,ij&quot;, 3, c)</span>
<span class="sd">    array([[ 0,  3,  6],</span>
<span class="sd">           [ 9, 12, 15]])</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(3, [Ellipsis], c, [Ellipsis])</span>
<span class="sd">    array([[ 0,  3,  6],</span>
<span class="sd">           [ 9, 12, 15]])</span>
<span class="sd">    &gt;&gt;&gt; np.multiply(3, c)</span>
<span class="sd">    array([[ 0,  3,  6],</span>
<span class="sd">           [ 9, 12, 15]])</span>

<span class="sd">    Vector outer product:</span>

<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;i,j&quot;, np.arange(2)+1, b)</span>
<span class="sd">    array([[0, 1, 2, 3, 4],</span>
<span class="sd">           [0, 2, 4, 6, 8]])</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(np.arange(2)+1, [0], b, [1])</span>
<span class="sd">    array([[0, 1, 2, 3, 4],</span>
<span class="sd">           [0, 2, 4, 6, 8]])</span>
<span class="sd">    &gt;&gt;&gt; np.outer(np.arange(2)+1, b)</span>
<span class="sd">    array([[0, 1, 2, 3, 4],</span>
<span class="sd">           [0, 2, 4, 6, 8]])</span>

<span class="sd">    Tensor contraction:</span>

<span class="sd">    &gt;&gt;&gt; a = np.arange(60.).reshape(3, 4, 5)</span>
<span class="sd">    &gt;&gt;&gt; b = np.arange(24.).reshape(4, 3, 2)</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;ijk,jil-&gt;kl&quot;, a, b)</span>
<span class="sd">    array([[4400., 4730.],</span>
<span class="sd">           [4532., 4874.],</span>
<span class="sd">           [4664., 5018.],</span>
<span class="sd">           [4796., 5162.],</span>
<span class="sd">           [4928., 5306.]])</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(a, [0, 1, 2], b, [1, 0, 3], [2, 3])</span>
<span class="sd">    array([[4400., 4730.],</span>
<span class="sd">           [4532., 4874.],</span>
<span class="sd">           [4664., 5018.],</span>
<span class="sd">           [4796., 5162.],</span>
<span class="sd">           [4928., 5306.]])</span>
<span class="sd">    &gt;&gt;&gt; np.tensordot(a, b, axes=([1, 0],[0, 1]))</span>
<span class="sd">    array([[4400., 4730.],</span>
<span class="sd">           [4532., 4874.],</span>
<span class="sd">           [4664., 5018.],</span>
<span class="sd">           [4796., 5162.],</span>
<span class="sd">           [4928., 5306.]])</span>

<span class="sd">    Example of ellipsis use:</span>

<span class="sd">    &gt;&gt;&gt; a = np.arange(6).reshape((3, 2))</span>
<span class="sd">    &gt;&gt;&gt; b = np.arange(12).reshape((4, 3))</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;ki,jk-&gt;ij&quot;, a, b)</span>
<span class="sd">    array([[10, 28, 46, 64],</span>
<span class="sd">           [13, 40, 67, 94]])</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;ki,...k-&gt;i...&quot;, a, b)</span>
<span class="sd">    array([[10, 28, 46, 64],</span>
<span class="sd">           [13, 40, 67, 94]])</span>
<span class="sd">    &gt;&gt;&gt; np.einsum(&quot;k...,jk&quot;, a, b)</span>
<span class="sd">    array([[10, 28, 46, 64],</span>
<span class="sd">           [13, 40, 67, 94]])</span>

<span class="sd">    Chained array operations. For more complicated contractions, speed ups</span>
<span class="sd">    might be achieved by repeatedly computing a &quot;greedy&quot; path or computing</span>
<span class="sd">    the &quot;optimal&quot; path in advance and repeatedly applying it, using an</span>
<span class="sd">    `einsum_path` insertion. Performance improvements can be particularly</span>
<span class="sd">    significant with larger arrays:</span>

<span class="sd">    &gt;&gt;&gt; a = np.ones(64000).reshape(20, 40, 80)</span>

<span class="sd">    Basic `einsum`: 119 ms ± 26 ms per loop (evaluated on 12th</span>
<span class="sd">    Gen Intel\u00ae Core\u2122 i7 processor)</span>

<span class="sd">    &gt;&gt;&gt; %timeit np.einsum(&quot;ijk,ilm,njm,nlk,abc-&gt;&quot;,a, a, a, a, a)</span>

<span class="sd">    Sub-optimal `einsum`: 32.9 ms ± 5.1 ms per loop</span>

<span class="sd">    &gt;&gt;&gt; %timeit np.einsum(</span>
<span class="sd">        &quot;ijk,ilm,njm,nlk,abc-&gt;&quot;, a, a, a, a, a, optimize=&quot;optimal&quot;</span>
<span class="sd">    )</span>

<span class="sd">    Greedy `einsum`: 28.6 ms ± 4.8 ms per loop</span>

<span class="sd">    &gt;&gt;&gt; %timeit np.einsum(</span>
<span class="sd">        &quot;ijk,ilm,njm,nlk,abc-&gt;&quot;, a, a, a, a, a, optimize=&quot;greedy&quot;</span>
<span class="sd">    )</span>

<span class="sd">    Optimal `einsum`: 26.9 ms ± 6.3 ms per loop</span>

<span class="sd">    &gt;&gt;&gt; path = np.einsum_path(</span>
<span class="sd">        &quot;ijk,ilm,njm,nlk,abc-&gt;&quot;, a, a, a, a, a, optimize=&quot;optimal&quot;</span>
<span class="sd">    )[0]</span>
<span class="sd">    &gt;&gt;&gt; %timeit np.einsum(&quot;ijk,ilm,njm,nlk,abc-&gt;&quot;, a, a, a, a, a, optimize=path)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">optimize</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">optimize</span> <span class="o">=</span> <span class="s2">&quot;greedy&quot;</span>

    <span class="k">return</span> <span class="n">dpnp_einsum</span><span class="p">(</span>
        <span class="o">*</span><span class="n">operands</span><span class="p">,</span>
        <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span>
        <span class="n">casting</span><span class="o">=</span><span class="n">casting</span><span class="p">,</span>
        <span class="n">optimize</span><span class="o">=</span><span class="n">optimize</span><span class="p">,</span>
    <span class="p">)</span></div>



<div class="viewcode-block" id="einsum_path">
<a class="viewcode-back" href="../../reference/generated/dpnp.einsum_path.html#dpnp.einsum_path">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">einsum_path</span><span class="p">(</span><span class="o">*</span><span class="n">operands</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    einsum_path(subscripts, *operands, optimize=&quot;greedy&quot;)</span>

<span class="sd">    Evaluates the lowest cost contraction order for an einsum expression</span>
<span class="sd">    by considering the creation of intermediate arrays.</span>

<span class="sd">    For full documentation refer to :obj:`numpy.einsum_path`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    subscripts : str</span>
<span class="sd">        Specifies the subscripts for summation.</span>
<span class="sd">    *operands : sequence of arrays</span>
<span class="sd">        These are the arrays for the operation in any form that can be</span>
<span class="sd">        converted to an array. This includes scalars, lists, lists of</span>
<span class="sd">        tuples, tuples, tuples of tuples, tuples of lists, and ndarrays.</span>
<span class="sd">    optimize : {bool, list, tuple, None, &quot;greedy&quot;, &quot;optimal&quot;}</span>
<span class="sd">        Choose the type of path. If a tuple is provided, the second argument is</span>
<span class="sd">        assumed to be the maximum intermediate size created. If only a single</span>
<span class="sd">        argument is provided the largest input or output array size is used</span>
<span class="sd">        as a maximum intermediate size.</span>

<span class="sd">        * if a list is given that starts with ``einsum_path``, uses this as the</span>
<span class="sd">          contraction path</span>
<span class="sd">        * if ``False`` or ``None`` no optimization is taken</span>
<span class="sd">        * if ``True`` defaults to the ``&quot;greedy&quot;`` algorithm</span>
<span class="sd">        * ``&quot;optimal&quot;`` is an algorithm that combinatorially explores all</span>
<span class="sd">          possible ways of contracting the listed tensors and chooses the</span>
<span class="sd">          least costly path. Scales exponentially with the number of terms</span>
<span class="sd">          in the contraction.</span>
<span class="sd">        * ``&quot;greedy&quot;`` is an algorithm that chooses the best pair contraction</span>
<span class="sd">          at each step. Effectively, this algorithm searches the largest inner,</span>
<span class="sd">          Hadamard, and then outer products at each step. Scales cubically with</span>
<span class="sd">          the number of terms in the contraction. Equivalent to the</span>
<span class="sd">          ``&quot;optimal&quot;`` path for most contractions.</span>

<span class="sd">        Default: ``&quot;greedy&quot;``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    path : list of tuples</span>
<span class="sd">        A list representation of the einsum path.</span>
<span class="sd">    string_repr : str</span>
<span class="sd">        A printable representation of the einsum path.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The resulting path indicates which terms of the input contraction should be</span>
<span class="sd">    contracted first, the result of this contraction is then appended to the</span>
<span class="sd">    end of the contraction list. This list can then be iterated over until all</span>
<span class="sd">    intermediate contractions are complete.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    :obj:`dpnp.einsum` : Evaluates the Einstein summation convention</span>
<span class="sd">                         on the operands.</span>
<span class="sd">    :obj:`dpnp.linalg.multi_dot` : Chained dot product.</span>
<span class="sd">    :obj:`dpnp.dot` : Returns the dot product of two arrays.</span>
<span class="sd">    :obj:`dpnp.inner` : Returns the inner product of two arrays.</span>
<span class="sd">    :obj:`dpnp.outer` : Returns the outer product of two arrays.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    We can begin with a chain dot example. In this case, it is optimal to</span>
<span class="sd">    contract the `b` and `c` tensors first as represented by the first element</span>
<span class="sd">    of the path ``(1, 2)``. The resulting tensor is added to the end of the</span>
<span class="sd">    contraction and the remaining contraction ``(0, 1)`` is then completed.</span>

<span class="sd">    &gt;&gt;&gt; import dpnp as np</span>
<span class="sd">    &gt;&gt;&gt; np.random.seed(123)</span>
<span class="sd">    &gt;&gt;&gt; a = np.random.rand(2, 2)</span>
<span class="sd">    &gt;&gt;&gt; b = np.random.rand(2, 5)</span>
<span class="sd">    &gt;&gt;&gt; c = np.random.rand(5, 2)</span>
<span class="sd">    &gt;&gt;&gt; path_info = np.einsum_path(&quot;ij,jk,kl-&gt;il&quot;, a, b, c, optimize=&quot;greedy&quot;)</span>

<span class="sd">    &gt;&gt;&gt; print(path_info[0])</span>
<span class="sd">    [&#39;einsum_path&#39;, (1, 2), (0, 1)]</span>

<span class="sd">    &gt;&gt;&gt; print(path_info[1])</span>
<span class="sd">      Complete contraction:  ij,jk,kl-&gt;il # may vary</span>
<span class="sd">             Naive scaling:  4</span>
<span class="sd">         Optimized scaling:  3</span>
<span class="sd">          Naive FLOP count:  1.200e+02</span>
<span class="sd">      Optimized FLOP count:  5.700e+01</span>
<span class="sd">       Theoretical speedup:  2.105</span>
<span class="sd">      Largest intermediate:  4.000e+00 elements</span>
<span class="sd">    -------------------------------------------------------------------------</span>
<span class="sd">    scaling                  current                                remaining</span>
<span class="sd">    -------------------------------------------------------------------------</span>
<span class="sd">       3                   kl,jk-&gt;jl                                ij,jl-&gt;il</span>
<span class="sd">       3                   jl,ij-&gt;il                                   il-&gt;il</span>

<span class="sd">    A more complex index transformation example.</span>

<span class="sd">    &gt;&gt;&gt; I = np.random.rand(10, 10, 10, 10)</span>
<span class="sd">    &gt;&gt;&gt; C = np.random.rand(10, 10)</span>
<span class="sd">    &gt;&gt;&gt; path_info = np.einsum_path(</span>
<span class="sd">            &quot;ea,fb,abcd,gc,hd-&gt;efgh&quot;, C, C, I, C, C, optimize=&quot;greedy&quot;</span>
<span class="sd">        )</span>
<span class="sd">    &gt;&gt;&gt; print(path_info[0])</span>
<span class="sd">    [&#39;einsum_path&#39;, (0, 2), (0, 3), (0, 2), (0, 1)]</span>
<span class="sd">    &gt;&gt;&gt; print(path_info[1])</span>
<span class="sd">      Complete contraction:  ea,fb,abcd,gc,hd-&gt;efgh # may vary</span>
<span class="sd">             Naive scaling:  8</span>
<span class="sd">         Optimized scaling:  5</span>
<span class="sd">          Naive FLOP count:  5.000e+08</span>
<span class="sd">      Optimized FLOP count:  8.000e+05</span>
<span class="sd">       Theoretical speedup:  624.999</span>
<span class="sd">      Largest intermediate:  1.000e+04 elements</span>
<span class="sd">    --------------------------------------------------------------------------</span>
<span class="sd">    scaling                  current                                remaining</span>
<span class="sd">    --------------------------------------------------------------------------</span>
<span class="sd">       5               abcd,ea-&gt;bcde                      fb,gc,hd,bcde-&gt;efgh</span>
<span class="sd">       5               bcde,fb-&gt;cdef                         gc,hd,cdef-&gt;efgh</span>
<span class="sd">       5               cdef,gc-&gt;defg                            hd,defg-&gt;efgh</span>
<span class="sd">       5               defg,hd-&gt;efgh                               efgh-&gt;efgh</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># explicit casting to numpy array if applicable</span>
    <span class="n">operands</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">dpnp</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">is_supported_array_type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">operands</span>
    <span class="p">]</span>

    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">einsum_path</span><span class="p">(</span>
        <span class="o">*</span><span class="n">operands</span><span class="p">,</span>
        <span class="n">optimize</span><span class="o">=</span><span class="n">optimize</span><span class="p">,</span>
        <span class="n">einsum_call</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span></div>



<div class="viewcode-block" id="inner">
<a class="viewcode-back" href="../../reference/generated/dpnp.inner.html#dpnp.inner">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">inner</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the inner product of two arrays.</span>

<span class="sd">    For full documentation refer to :obj:`numpy.inner`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : {dpnp.ndarray, usm_ndarray, scalar}</span>
<span class="sd">        First input array. Both inputs `a` and `b` can not be scalars</span>
<span class="sd">        at the same time.</span>
<span class="sd">    b : {dpnp.ndarray, usm_ndarray, scalar}</span>
<span class="sd">        Second input array. Both inputs `a` and `b` can not be scalars</span>
<span class="sd">        at the same time.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : dpnp.ndarray</span>
<span class="sd">        If either `a` or `b` is a scalar, the shape of the returned arrays</span>
<span class="sd">        matches that of the array between `a` and `b`, whichever is an array.</span>
<span class="sd">        If `a` and `b` are both 1-D arrays then a 0-d array is returned;</span>
<span class="sd">        otherwise an array with a shape as</span>
<span class="sd">        ``out.shape = (*a.shape[:-1], *b.shape[:-1])`` is returned.</span>


<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    :obj:`dpnp.einsum` : Einstein summation convention.</span>
<span class="sd">    :obj:`dpnp.dot` : Generalized matrix product,</span>
<span class="sd">                      using second last dimension of `b`.</span>
<span class="sd">    :obj:`dpnp.tensordot` : Sum products over arbitrary axes.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    # Ordinary inner product for vectors</span>

<span class="sd">    &gt;&gt;&gt; import dpnp as np</span>
<span class="sd">    &gt;&gt;&gt; a = np.array([1, 2, 3])</span>
<span class="sd">    &gt;&gt;&gt; b = np.array([0, 1, 0])</span>
<span class="sd">    &gt;&gt;&gt; np.inner(a, b)</span>
<span class="sd">    array(2)</span>

<span class="sd">    # Some multidimensional examples</span>

<span class="sd">    &gt;&gt;&gt; a = np.arange(24).reshape((2, 3, 4))</span>
<span class="sd">    &gt;&gt;&gt; b = np.arange(4)</span>
<span class="sd">    &gt;&gt;&gt; c = np.inner(a, b)</span>
<span class="sd">    &gt;&gt;&gt; c.shape</span>
<span class="sd">    (2, 3)</span>
<span class="sd">    &gt;&gt;&gt; c</span>
<span class="sd">    array([[ 14,  38,  62],</span>
<span class="sd">           [86, 110, 134]])</span>

<span class="sd">    &gt;&gt;&gt; a = np.arange(2).reshape((1, 1, 2))</span>
<span class="sd">    &gt;&gt;&gt; b = np.arange(6).reshape((3, 2))</span>
<span class="sd">    &gt;&gt;&gt; c = np.inner(a, b)</span>
<span class="sd">    &gt;&gt;&gt; c.shape</span>
<span class="sd">    (1, 1, 3)</span>
<span class="sd">    &gt;&gt;&gt; c</span>
<span class="sd">    array([[[1, 3, 5]]])</span>

<span class="sd">    An example where `b` is a scalar</span>

<span class="sd">    &gt;&gt;&gt; np.inner(np.eye(2), 7)</span>
<span class="sd">    array([[7., 0.],</span>
<span class="sd">           [0., 7.]])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dpnp</span><span class="o">.</span><span class="n">check_supported_arrays_type</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">scalar_type</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">or</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_call_multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">b</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># TODO: use specific scalar-vector kernel</span>
        <span class="k">return</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;shape of input arrays is not similar at the last axis.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">b</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dpnp_dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span></div>



<div class="viewcode-block" id="kron">
<a class="viewcode-back" href="../../reference/generated/dpnp.kron.html#dpnp.kron">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">kron</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Kronecker product of two arrays.</span>

<span class="sd">    Computes the Kronecker product, a composite array made of blocks of the</span>
<span class="sd">    second array scaled by the first.</span>

<span class="sd">    For full documentation refer to :obj:`numpy.kron`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : {dpnp.ndarray, usm_ndarray, scalar}</span>
<span class="sd">        First input array. Both inputs `a` and `b` can not be scalars</span>
<span class="sd">        at the same time.</span>
<span class="sd">    b : {dpnp.ndarray, usm_ndarray, scalar}</span>
<span class="sd">        Second input array. Both inputs `a` and `b` can not be scalars</span>
<span class="sd">        at the same time.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : dpnp.ndarray</span>
<span class="sd">        Returns the Kronecker product.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    :obj:`dpnp.outer` : Returns the outer product of two arrays.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import dpnp as np</span>
<span class="sd">    &gt;&gt;&gt; a = np.array([1, 10, 100])</span>
<span class="sd">    &gt;&gt;&gt; b = np.array([5, 6, 7])</span>
<span class="sd">    &gt;&gt;&gt; np.kron(a, b)</span>
<span class="sd">    array([  5,   6,   7, ..., 500, 600, 700])</span>
<span class="sd">    &gt;&gt;&gt; np.kron(b, a)</span>
<span class="sd">    array([  5,  50, 500, ...,   7,  70, 700])</span>

<span class="sd">    &gt;&gt;&gt; np.kron(np.eye(2), np.ones((2, 2)))</span>
<span class="sd">    array([[1.,  1.,  0.,  0.],</span>
<span class="sd">           [1.,  1.,  0.,  0.],</span>
<span class="sd">           [0.,  0.,  1.,  1.],</span>
<span class="sd">           [0.,  0.,  1.,  1.]])</span>

<span class="sd">    &gt;&gt;&gt; a = np.arange(100).reshape((2, 5, 2, 5))</span>
<span class="sd">    &gt;&gt;&gt; b = np.arange(24).reshape((2, 3, 4))</span>
<span class="sd">    &gt;&gt;&gt; c = np.kron(a, b)</span>
<span class="sd">    &gt;&gt;&gt; c.shape</span>
<span class="sd">    (2, 10, 6, 20)</span>
<span class="sd">    &gt;&gt;&gt; I = (1, 3, 0, 2)</span>
<span class="sd">    &gt;&gt;&gt; J = (0, 2, 1)</span>
<span class="sd">    &gt;&gt;&gt; J1 = (0,) + J             # extend to ndim=4</span>
<span class="sd">    &gt;&gt;&gt; S1 = (1,) + b.shape</span>
<span class="sd">    &gt;&gt;&gt; K = tuple(np.array(I) * np.array(S1) + np.array(J1))</span>
<span class="sd">    &gt;&gt;&gt; c[K] == a[I]*b[J]</span>
<span class="sd">    array(True)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dpnp</span><span class="o">.</span><span class="n">check_supported_arrays_type</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">scalar_type</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">or</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_call_multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="n">a_ndim</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">b_ndim</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">ndim</span>
    <span class="k">if</span> <span class="n">a_ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">b_ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># TODO: use specific scalar-vector kernel</span>
        <span class="k">return</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dpnp_kron</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">a_ndim</span><span class="p">,</span> <span class="n">b_ndim</span><span class="p">)</span></div>



<div class="viewcode-block" id="matmul">
<a class="viewcode-back" href="../../reference/generated/dpnp.matmul.html#dpnp.matmul">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">matmul</span><span class="p">(</span>
    <span class="n">x1</span><span class="p">,</span>
    <span class="n">x2</span><span class="p">,</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">casting</span><span class="o">=</span><span class="s2">&quot;same_kind&quot;</span><span class="p">,</span>
    <span class="n">order</span><span class="o">=</span><span class="s2">&quot;K&quot;</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">subok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">signature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Matrix product of two arrays.</span>

<span class="sd">    For full documentation refer to :obj:`numpy.matmul`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x1 : {dpnp.ndarray, usm_ndarray}</span>
<span class="sd">        First input array.</span>
<span class="sd">    x2 : {dpnp.ndarray, usm_ndarray}</span>
<span class="sd">        Second input array.</span>
<span class="sd">    out : {None, dpnp.ndarray, usm_ndarray}, optional</span>
<span class="sd">        Alternative output array in which to place the result. It must have</span>
<span class="sd">        a shape that matches the signature `(n,k),(k,m)-&gt;(n,m)` but the type</span>
<span class="sd">        (of the calculated values) will be cast if necessary.</span>

<span class="sd">        Default: ``None``.</span>
<span class="sd">    dtype : {None, str, dtype object}, optional</span>
<span class="sd">        Type to use in computing the matrix product. By default, the returned</span>
<span class="sd">        array will have data type that is determined by considering</span>
<span class="sd">        Promotion Type Rule and device capabilities.</span>

<span class="sd">        Default: ``None``.</span>
<span class="sd">    casting : {&quot;no&quot;, &quot;equiv&quot;, &quot;safe&quot;, &quot;same_kind&quot;, &quot;unsafe&quot;}, optional</span>
<span class="sd">        Controls what kind of data casting may occur.</span>

<span class="sd">        Default: ``&quot;same_kind&quot;``.</span>
<span class="sd">    order : {None, &quot;C&quot;, &quot;F&quot;, &quot;A&quot;, &quot;K&quot;}, optional</span>
<span class="sd">        Memory layout of the newly output array, if parameter `out` is ``None``.</span>

<span class="sd">        Default: ``&quot;K&quot;``.</span>
<span class="sd">    axes : {None, list of tuples}, optional</span>
<span class="sd">        A list of tuples with indices of axes the matrix product should operate</span>
<span class="sd">        on. For instance, for the signature of ``(i,j),(j,k)-&gt;(i,k)``, the base</span>
<span class="sd">        elements are 2d matrices and these are taken to be stored in the two</span>
<span class="sd">        last axes of each argument. The corresponding axes keyword would be</span>
<span class="sd">        ``[(-2, -1), (-2, -1), (-2, -1)]``.</span>

<span class="sd">        Default: ``None``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : dpnp.ndarray</span>
<span class="sd">        Returns the matrix product of the inputs.</span>
<span class="sd">        This is a 0-d array only when both `x1`, `x2` are 1-d vectors.</span>

<span class="sd">    Limitations</span>
<span class="sd">    -----------</span>
<span class="sd">    Keyword arguments `subok`, and `signature`, are only supported with their</span>
<span class="sd">    default values. Otherwise ``NotImplementedError`` exception will be raised.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    :obj:`dpnp.linalg.matmul` : Array API compatible version.</span>
<span class="sd">    :obj:`dpnp.vecdot` : Complex-conjugating dot product for stacks of vectors.</span>
<span class="sd">    :obj:`dpnp.matvec` : Matrix-vector product for stacks of</span>
<span class="sd">                         matrices and vectors.</span>
<span class="sd">    :obj:`dpnp.vecmat` : Vector-matrix product for stacks of</span>
<span class="sd">                         vectors and matrices.</span>
<span class="sd">    :obj:`dpnp.tensordot` : Sum products over arbitrary axes.</span>
<span class="sd">    :obj:`dpnp.einsum` : Einstein summation convention.</span>
<span class="sd">    :obj:`dpnp.dot` : Alternative matrix product with</span>
<span class="sd">                      different broadcasting rules.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The behavior depends on the arguments in the following way.</span>

<span class="sd">    - If both arguments are 2-D they are multiplied like conventional matrices.</span>
<span class="sd">    - If either argument is N-D, N &gt; 2, it is treated as a stack of matrices</span>
<span class="sd">      residing in the last two indexes and broadcast accordingly.</span>
<span class="sd">    - If the first argument is 1-D, it is promoted to a matrix by prepending</span>
<span class="sd">      a 1 to its dimensions. After matrix multiplication the prepended 1 is</span>
<span class="sd">      removed. (For stacks of vectors, use :obj:`dpnp.vecmat`.)</span>
<span class="sd">    - If the second argument is 1-D, it is promoted to a matrix by appending</span>
<span class="sd">      a 1 to its dimensions. After matrix multiplication the appended 1 is</span>
<span class="sd">      removed. (For stacks of vectors, use :obj:`dpnp.matvec`.)</span>

<span class="sd">    :obj:`dpnp.matmul` differs from :obj:`dpnp.dot` in two important ways:</span>

<span class="sd">    - Multiplication by scalars is not allowed, use ``*`` instead.</span>
<span class="sd">    - Stacks of matrices are broadcast together as if the matrices</span>
<span class="sd">      were elements, respecting the signature ``(n,k),(k,m)-&gt;(n,m)``:</span>

<span class="sd">    &gt;&gt;&gt; import dpnp as np</span>
<span class="sd">    &gt;&gt;&gt; a = np.ones([9, 5, 7, 4])</span>
<span class="sd">    &gt;&gt;&gt; c = np.ones([9, 5, 4, 3])</span>
<span class="sd">    &gt;&gt;&gt; np.dot(a, c).shape</span>
<span class="sd">    (9, 5, 7, 9, 5, 3)</span>
<span class="sd">    &gt;&gt;&gt; np.matmul(a, c).shape</span>
<span class="sd">    (9, 5, 7, 3) # n is 7, k is 4, m is 3</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    For 2-D arrays it is the matrix product:</span>

<span class="sd">    &gt;&gt;&gt; import dpnp as np</span>
<span class="sd">    &gt;&gt;&gt; a = np.array([[1, 0], [0, 1]])</span>
<span class="sd">    &gt;&gt;&gt; b = np.array([[4, 1], [2, 2]])</span>
<span class="sd">    &gt;&gt;&gt; np.matmul(a, b)</span>
<span class="sd">    array([[4, 1],</span>
<span class="sd">           [2, 2]])</span>

<span class="sd">    For 2-D mixed with 1-D, the result is the usual.</span>

<span class="sd">    &gt;&gt;&gt; a = np.array([[1, 0], [0, 1]])</span>
<span class="sd">    &gt;&gt;&gt; b = np.array([1, 2])</span>
<span class="sd">    &gt;&gt;&gt; np.matmul(a, b)</span>
<span class="sd">    array([1, 2])</span>
<span class="sd">    &gt;&gt;&gt; np.matmul(b, a)</span>
<span class="sd">    array([1, 2])</span>

<span class="sd">    Broadcasting is conventional for stacks of arrays</span>

<span class="sd">    &gt;&gt;&gt; a = np.arange(2 * 2 * 4).reshape((2, 2, 4))</span>
<span class="sd">    &gt;&gt;&gt; b = np.arange(2 * 2 * 4).reshape((2, 4, 2))</span>
<span class="sd">    &gt;&gt;&gt; np.matmul(a, b).shape</span>
<span class="sd">    (2, 2, 2)</span>
<span class="sd">    &gt;&gt;&gt; np.matmul(a, b)[0, 1, 1]</span>
<span class="sd">    array(98)</span>
<span class="sd">    &gt;&gt;&gt; np.sum(a[0, 1, :] * b[0 , :, 1])</span>
<span class="sd">    array(98)</span>

<span class="sd">    Vector, vector returns the scalar inner product, but neither argument</span>
<span class="sd">    is complex-conjugated:</span>

<span class="sd">    &gt;&gt;&gt; x1 = np.array([2j, 3j])</span>
<span class="sd">    &gt;&gt;&gt; x2 = np.array([2j, 3j])</span>
<span class="sd">    &gt;&gt;&gt; np.matmul(x1, x2)</span>
<span class="sd">    array(-13+0j)</span>

<span class="sd">    The ``@`` operator can be used as a shorthand for :obj:`dpnp.matmul` on</span>
<span class="sd">    :class:`dpnp.ndarray`.</span>

<span class="sd">    &gt;&gt;&gt; x1 @ x2</span>
<span class="sd">    array(-13+0j)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dpnp</span><span class="o">.</span><span class="n">check_limitations</span><span class="p">(</span><span class="n">subok_linalg</span><span class="o">=</span><span class="n">subok</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">)</span>
    <span class="n">dpnp</span><span class="o">.</span><span class="n">check_supported_arrays_type</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dpnp_multiplication</span><span class="p">(</span>
        <span class="s2">&quot;matmul&quot;</span><span class="p">,</span>
        <span class="n">x1</span><span class="p">,</span>
        <span class="n">x2</span><span class="p">,</span>
        <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span>
        <span class="n">casting</span><span class="o">=</span><span class="n">casting</span><span class="p">,</span>
        <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span>
        <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
    <span class="p">)</span></div>



<div class="viewcode-block" id="matvec">
<a class="viewcode-back" href="../../reference/generated/dpnp.matvec.html#dpnp.matvec">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">matvec</span><span class="p">(</span>
    <span class="n">x1</span><span class="p">,</span>
    <span class="n">x2</span><span class="p">,</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">casting</span><span class="o">=</span><span class="s2">&quot;same_kind&quot;</span><span class="p">,</span>
    <span class="n">order</span><span class="o">=</span><span class="s2">&quot;K&quot;</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">subok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">signature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Matrix-vector dot product of two arrays.</span>

<span class="sd">    Given a matrix (or stack of matrices) :math:`\mathbf{A}` in `x1` and</span>
<span class="sd">    a vector (or stack of vectors) :math:`\mathbf{v}` in `x2`, the</span>
<span class="sd">    matrix-vector product is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">       \mathbf{A} \cdot \mathbf{v} = \sum_{j=0}^{n-1} A_{ij} v_j</span>

<span class="sd">    where the sum is over the last dimensions in `x1` and `x2` (unless `axes`</span>
<span class="sd">    is specified). (For a matrix-vector product with the vector conjugated,</span>
<span class="sd">    use ``dpnp.vecmat(x2, x1.mT)``.)</span>

<span class="sd">    For full documentation refer to :obj:`numpy.matvec`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x1 : {dpnp.ndarray, usm_ndarray}</span>
<span class="sd">        First input array.</span>
<span class="sd">    x2 : {dpnp.ndarray, usm_ndarray}</span>
<span class="sd">        Second input array.</span>
<span class="sd">    out : {None, dpnp.ndarray, usm_ndarray}, optional</span>
<span class="sd">        A location into which the result is stored. If provided, it must have</span>
<span class="sd">        the broadcasted shape of `x1` and `x2` with the summation axis removed.</span>
<span class="sd">        If not provided or ``None``, a freshly-allocated array is used.</span>

<span class="sd">        Default: ``None``.</span>
<span class="sd">    dtype : {None, str, dtype object}, optional</span>
<span class="sd">        Type to use in computing the matrix product. By default, the returned</span>
<span class="sd">        array will have data type that is determined by considering</span>
<span class="sd">        Promotion Type Rule and device capabilities.</span>

<span class="sd">        Default: ``None``.</span>
<span class="sd">    casting : {&quot;no&quot;, &quot;equiv&quot;, &quot;safe&quot;, &quot;same_kind&quot;, &quot;unsafe&quot;}, optional</span>
<span class="sd">        Controls what kind of data casting may occur.</span>

<span class="sd">        Default: ``&quot;same_kind&quot;``.</span>
<span class="sd">    order : {None, &quot;C&quot;, &quot;F&quot;, &quot;A&quot;, &quot;K&quot;}, optional</span>
<span class="sd">        Memory layout of the newly output array, if parameter `out` is ``None``.</span>

<span class="sd">        Default: ``&quot;K&quot;``.</span>
<span class="sd">    axes : {None, list of tuples}, optional</span>
<span class="sd">        A list of tuples with indices of axes the matrix-vector product should</span>
<span class="sd">        operate on. For instance, for the signature of ``(i,j),(j)-&gt;(i)``, the</span>
<span class="sd">        base elements are 2d matrices and 1d vectors, where the matrices are</span>
<span class="sd">        assumed to be stored in the last two axes of the first argument, and</span>
<span class="sd">        the vectors in the last axis of the second argument. The corresponding</span>
<span class="sd">        axes keyword would be ``[(-2, -1), (-1,), (-1,)]``.</span>

<span class="sd">        Default: ``None``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : dpnp.ndarray</span>
<span class="sd">        Returns the matrix-vector product of the inputs.</span>

<span class="sd">    Limitations</span>
<span class="sd">    -----------</span>
<span class="sd">    Keyword arguments `subok`, and `signature` are only supported with their</span>
<span class="sd">    default values. Otherwise ``NotImplementedError`` exception will be raised.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    :obj:`dpnp.vecdot` : Vector-vector product.</span>
<span class="sd">    :obj:`dpnp.vecmat` : Vector-matrix product.</span>
<span class="sd">    :obj:`dpnp.matmul` : Matrix-matrix product.</span>
<span class="sd">    :obj:`dpnp.einsum` : Einstein summation convention.</span>


<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Rotate a set of vectors from Y to X along Z.</span>

<span class="sd">    &gt;&gt;&gt; import dpnp as np</span>
<span class="sd">    &gt;&gt;&gt; a = np.array([[0., 1., 0.],</span>
<span class="sd">    ...               [-1., 0., 0.],</span>
<span class="sd">    ...               [0., 0., 1.]])</span>
<span class="sd">    &gt;&gt;&gt; v = np.array([[1., 0., 0.],</span>
<span class="sd">    ...               [0., 1., 0.],</span>
<span class="sd">    ...               [0., 0., 1.],</span>
<span class="sd">    ...               [0., 6., 8.]])</span>
<span class="sd">    &gt;&gt;&gt; np.matvec(a, v)</span>
<span class="sd">    array([[ 0., -1.,  0.],</span>
<span class="sd">           [ 1.,  0.,  0.],</span>
<span class="sd">           [ 0.,  0.,  1.],</span>
<span class="sd">           [ 6.,  0.,  8.]])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dpnp</span><span class="o">.</span><span class="n">check_limitations</span><span class="p">(</span><span class="n">subok_linalg</span><span class="o">=</span><span class="n">subok</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">)</span>
    <span class="n">dpnp</span><span class="o">.</span><span class="n">check_supported_arrays_type</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dpnp_multiplication</span><span class="p">(</span>
        <span class="s2">&quot;matvec&quot;</span><span class="p">,</span>
        <span class="n">x1</span><span class="p">,</span>
        <span class="n">x2</span><span class="p">,</span>
        <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span>
        <span class="n">casting</span><span class="o">=</span><span class="n">casting</span><span class="p">,</span>
        <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span>
        <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
    <span class="p">)</span></div>



<div class="viewcode-block" id="outer">
<a class="viewcode-back" href="../../reference/generated/dpnp.outer.html#dpnp.outer">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">outer</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the outer product of two arrays.</span>

<span class="sd">    For full documentation refer to :obj:`numpy.outer`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : {dpnp.ndarray, usm_ndarray}</span>
<span class="sd">        First input vector. Input is flattened if not already 1-dimensional.</span>
<span class="sd">    b : {dpnp.ndarray, usm_ndarray}</span>
<span class="sd">        Second input vector. Input is flattened if not already 1-dimensional.</span>
<span class="sd">    out : {None, dpnp.ndarray, usm_ndarray}, optional</span>
<span class="sd">        A location where the result is stored.</span>

<span class="sd">        Default: ``None``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : dpnp.ndarray</span>
<span class="sd">        ``out[i, j] = a[i] * b[j]``</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    :obj:`dpnp.linalg.outer` : Array API compatible version.</span>
<span class="sd">    :obj:`dpnp.einsum` : Evaluates the Einstein summation convention</span>
<span class="sd">                         on the operands.</span>
<span class="sd">    :obj:`dpnp.inner` : Returns the inner product of two arrays.</span>
<span class="sd">    :obj:`dpnp.tensordot` : dpnp.tensordot(a.ravel(), b.ravel(), axes=((), ()))</span>
<span class="sd">                            is the equivalent.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import dpnp as np</span>
<span class="sd">    &gt;&gt;&gt; a = np.array([1, 1, 1])</span>
<span class="sd">    &gt;&gt;&gt; b = np.array([1, 2, 3])</span>
<span class="sd">    &gt;&gt;&gt; np.outer(a, b)</span>
<span class="sd">    array([[1, 2, 3],</span>
<span class="sd">           [1, 2, 3],</span>
<span class="sd">           [1, 2, 3]])</span>

<span class="sd">    Make a (*very* coarse) grid for computing a Mandelbrot set:</span>

<span class="sd">    &gt;&gt;&gt; rl = np.outer(np.ones((5,)), np.linspace(-2, 2, 5))</span>
<span class="sd">    &gt;&gt;&gt; rl</span>
<span class="sd">    array([[-2., -1.,  0.,  1.,  2.],</span>
<span class="sd">           [-2., -1.,  0.,  1.,  2.],</span>
<span class="sd">           [-2., -1.,  0.,  1.,  2.],</span>
<span class="sd">           [-2., -1.,  0.,  1.,  2.],</span>
<span class="sd">           [-2., -1.,  0.,  1.,  2.]])</span>
<span class="sd">    &gt;&gt;&gt; im = np.outer(1j*np.linspace(2, -2, 5), np.ones((5,)))</span>
<span class="sd">    &gt;&gt;&gt; im</span>
<span class="sd">    array([[0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j],</span>
<span class="sd">           [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],</span>
<span class="sd">           [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],</span>
<span class="sd">           [0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j],</span>
<span class="sd">           [0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j]])</span>
<span class="sd">    &gt;&gt;&gt; grid = rl + im</span>
<span class="sd">    &gt;&gt;&gt; grid</span>
<span class="sd">    array([[-2.+2.j, -1.+2.j,  0.+2.j,  1.+2.j,  2.+2.j],</span>
<span class="sd">           [-2.+1.j, -1.+1.j,  0.+1.j,  1.+1.j,  2.+1.j],</span>
<span class="sd">           [-2.+0.j, -1.+0.j,  0.+0.j,  1.+0.j,  2.+0.j],</span>
<span class="sd">           [-2.-1.j, -1.-1.j,  0.-1.j,  1.-1.j,  2.-1.j],</span>
<span class="sd">           [-2.-2.j, -1.-2.j,  0.-2.j,  1.-2.j,  2.-2.j]])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dpnp</span><span class="o">.</span><span class="n">check_supported_arrays_type</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">scalar_type</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">all_scalars</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">b</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_call_multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span> <span class="n">outer_calc</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">a</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_call_multiply</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span> <span class="n">outer_calc</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">multiply</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">dpnp</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">result</span></div>



<div class="viewcode-block" id="tensordot">
<a class="viewcode-back" href="../../reference/generated/dpnp.tensordot.html#dpnp.tensordot">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">tensordot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute tensor dot product along specified axes.</span>

<span class="sd">    Given two tensors, `a` and `b`, and an array_like object containing</span>
<span class="sd">    two array_like objects, ``(a_axes, b_axes)``, sum the products of</span>
<span class="sd">    `a`&#39;s and `b`&#39;s elements (components) over the axes specified by</span>
<span class="sd">    ``a_axes`` and ``b_axes``. The third argument can be a single non-negative</span>
<span class="sd">    integer_like scalar, ``N``; if it is such, then the last ``N`` dimensions</span>
<span class="sd">    of `a` and the first ``N`` dimensions of `b` are summed over.</span>

<span class="sd">    For full documentation refer to :obj:`numpy.tensordot`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : {dpnp.ndarray, usm_ndarray, scalar}</span>
<span class="sd">        First input array. Both inputs `a` and `b` can not be scalars</span>
<span class="sd">        at the same time.</span>
<span class="sd">    b : {dpnp.ndarray, usm_ndarray, scalar}</span>
<span class="sd">        Second input array. Both inputs `a` and `b` can not be scalars</span>
<span class="sd">        at the same time.</span>
<span class="sd">    axes : int or (2,) array_like</span>
<span class="sd">        * integer_like: If an int `N`, sum over the last `N` axes of `a` and</span>
<span class="sd">          the first `N` axes of `b` in order. The sizes of the corresponding</span>
<span class="sd">          axes must match.</span>
<span class="sd">        * (2,) array_like: A list of axes to be summed over, first sequence</span>
<span class="sd">          applying to `a`, second to `b`. Both elements array_like must be of</span>
<span class="sd">          the same length.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : dpnp.ndarray</span>
<span class="sd">        Returns the tensor dot product of `a` and `b`.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    :obj:`dpnp.linalg.tensordot` : Equivalent function.</span>
<span class="sd">    :obj:`dpnp.dot` : Returns the dot product.</span>
<span class="sd">    :obj:`dpnp.einsum` : Evaluates the Einstein summation convention</span>
<span class="sd">                         on the operands.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Three common use cases are:</span>
<span class="sd">        * ``axes = 0`` : tensor product :math:`a \otimes b`</span>
<span class="sd">        * ``axes = 1`` : tensor dot product :math:`a \cdot b`</span>
<span class="sd">        * ``axes = 2`` : (default) tensor double contraction :math:`a:b`</span>

<span class="sd">    When `axes` is integer, the sequence for evaluation will be: first</span>
<span class="sd">    the -Nth axis in `a` and 0th axis in `b`, and the -1th axis in `a` and</span>
<span class="sd">    Nth axis in `b` last.</span>

<span class="sd">    When there is more than one axis to sum over - and they are not the last</span>
<span class="sd">    (first) axes of `a` (`b`) - the argument `axes` should consist of</span>
<span class="sd">    two sequences of the same length, with the first axis to sum over given</span>
<span class="sd">    first in both sequences, the second axis second, and so forth.</span>

<span class="sd">    The shape of the result consists of the non-contracted axes of the</span>
<span class="sd">    first tensor, followed by the non-contracted axes of the second.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import dpnp as np</span>
<span class="sd">    &gt;&gt;&gt; a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</span>
<span class="sd">    &gt;&gt;&gt; b = np.array([1, 2, 3])</span>
<span class="sd">    &gt;&gt;&gt; np.tensordot(a, b, 1)</span>
<span class="sd">    array([14, 32, 50])</span>

<span class="sd">    &gt;&gt;&gt; a = np.arange(60.).reshape(3, 4, 5)</span>
<span class="sd">    &gt;&gt;&gt; b = np.arange(24.).reshape(4, 3, 2)</span>
<span class="sd">    &gt;&gt;&gt; c = np.tensordot(a, b, axes=([1, 0], [0, 1]))</span>
<span class="sd">    &gt;&gt;&gt; c.shape</span>
<span class="sd">    (5, 2)</span>
<span class="sd">    &gt;&gt;&gt; c</span>
<span class="sd">    array([[4400., 4730.],</span>
<span class="sd">           [4532., 4874.],</span>
<span class="sd">           [4664., 5018.],</span>
<span class="sd">           [4796., 5162.],</span>
<span class="sd">           [4928., 5306.]])</span>

<span class="sd">    A slower but equivalent way of computing the same...</span>

<span class="sd">    &gt;&gt;&gt; d = np.zeros((5, 2))</span>
<span class="sd">    &gt;&gt;&gt; for i in range(5):</span>
<span class="sd">    ...   for j in range(2):</span>
<span class="sd">    ...     for k in range(3):</span>
<span class="sd">    ...       for n in range(4):</span>
<span class="sd">    ...         d[i, j] += a[k, n, i] * b[n, k, j]</span>
<span class="sd">    &gt;&gt;&gt; c == d</span>
<span class="sd">    array([[ True,  True],</span>
<span class="sd">           [ True,  True],</span>
<span class="sd">           [ True,  True],</span>
<span class="sd">           [ True,  True],</span>
<span class="sd">           [ True,  True]])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dpnp</span><span class="o">.</span><span class="n">check_supported_arrays_type</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">scalar_type</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">or</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">axes</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;One of the inputs is scalar, axes should be zero.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">_call_multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dpnp_tensordot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">)</span></div>



<div class="viewcode-block" id="vdot">
<a class="viewcode-back" href="../../reference/generated/dpnp.vdot.html#dpnp.vdot">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">vdot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the dot product of two vectors.</span>

<span class="sd">    For full documentation refer to :obj:`numpy.dot`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : {dpnp.ndarray, usm_ndarray, scalar}</span>
<span class="sd">        First input array. Both inputs `a` and `b` can not be</span>
<span class="sd">        scalars at the same time. If `a` is complex, the complex</span>
<span class="sd">        conjugate is taken before the calculation of the dot product.</span>
<span class="sd">    b : {dpnp.ndarray, usm_ndarray, scalar}</span>
<span class="sd">        Second input array. Both inputs `a` and `b` can not be</span>
<span class="sd">        scalars at the same time.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : dpnp.ndarray</span>
<span class="sd">        Returns the dot product of `a` and `b`.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    :obj:`dpnp.dot` : Returns the dot product.</span>
<span class="sd">    :obj:`dpnp.matmul` : Returns the matrix product.</span>
<span class="sd">    :obj:`dpnp.vecdot` : Vector dot product of two arrays.</span>
<span class="sd">    :obj:`dpnp.linalg.vecdot` : Array API compatible version of</span>
<span class="sd">                    :obj:`dpnp.vecdot`.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import dpnp as np</span>
<span class="sd">    &gt;&gt;&gt; a = np.array([1+2j,3+4j])</span>
<span class="sd">    &gt;&gt;&gt; b = np.array([5+6j,7+8j])</span>
<span class="sd">    &gt;&gt;&gt; np.vdot(a, b)</span>
<span class="sd">    array(70-8j)</span>
<span class="sd">    &gt;&gt;&gt; np.vdot(b, a)</span>
<span class="sd">    array(70+8j)</span>

<span class="sd">    Note that higher-dimensional arrays are flattened!</span>

<span class="sd">    &gt;&gt;&gt; a = np.array([[1, 4], [5, 6]])</span>
<span class="sd">    &gt;&gt;&gt; b = np.array([[4, 1], [2, 2]])</span>
<span class="sd">    &gt;&gt;&gt; np.vdot(a, b)</span>
<span class="sd">    array(30)</span>
<span class="sd">    &gt;&gt;&gt; np.vdot(b, a)</span>
<span class="sd">    array(30)</span>
<span class="sd">    &gt;&gt;&gt; 1*4 + 4*1 + 5*2 + 6*2</span>
<span class="sd">    30</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dpnp</span><span class="o">.</span><span class="n">check_supported_arrays_type</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">scalar_type</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The second array should be of size one.&quot;</span><span class="p">)</span>
        <span class="n">a_conj</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">conj</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">_call_multiply</span><span class="p">(</span><span class="n">a_conj</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The first array should be of size one.&quot;</span><span class="p">)</span>
        <span class="n">a_conj</span> <span class="o">=</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">conj</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">_call_multiply</span><span class="p">(</span><span class="n">a_conj</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">b</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dpnp_dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">conjugate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># dot product of flatten arrays</span>
    <span class="k">return</span> <span class="n">dpnp_dot</span><span class="p">(</span><span class="n">dpnp</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">conjugate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>



<div class="viewcode-block" id="vecdot">
<a class="viewcode-back" href="../../reference/generated/dpnp.vecdot.html#dpnp.vecdot">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">vecdot</span><span class="p">(</span>
    <span class="n">x1</span><span class="p">,</span>
    <span class="n">x2</span><span class="p">,</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">casting</span><span class="o">=</span><span class="s2">&quot;same_kind&quot;</span><span class="p">,</span>
    <span class="n">order</span><span class="o">=</span><span class="s2">&quot;K&quot;</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">subok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">signature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the vector dot product.</span>

<span class="sd">    Let :math:`\mathbf{a}` be a vector in `x1` and :math:`\mathbf{b}` be</span>
<span class="sd">    a corresponding vector in `x2`. The dot product is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">       \mathbf{a} \cdot \mathbf{b} = \sum_{i=0}^{n-1} \overline{a_i}b_i</span>

<span class="sd">    where the sum is over the last dimension (unless `axis` is specified) and</span>
<span class="sd">    where :math:`\overline{a_i}` denotes the complex conjugate if :math:`a_i`</span>
<span class="sd">    is complex and the identity otherwise.</span>

<span class="sd">    For full documentation refer to :obj:`numpy.vecdot`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x1 : {dpnp.ndarray, usm_ndarray}</span>
<span class="sd">        First input array.</span>
<span class="sd">    x2 : {dpnp.ndarray, usm_ndarray}</span>
<span class="sd">        Second input array.</span>
<span class="sd">    out : {None, dpnp.ndarray, usm_ndarray}, optional</span>
<span class="sd">        A location into which the result is stored. If provided, it must have</span>
<span class="sd">        the broadcasted shape of `x1` and `x2` with the last axis removed.</span>
<span class="sd">        If not provided or ``None``, a freshly-allocated array is used.</span>

<span class="sd">        Default: ``None``.</span>
<span class="sd">    casting : {&quot;no&quot;, &quot;equiv&quot;, &quot;safe&quot;, &quot;same_kind&quot;, &quot;unsafe&quot;}, optional</span>
<span class="sd">        Controls what kind of data casting may occur.</span>

<span class="sd">        Default: ``&quot;same_kind&quot;``.</span>
<span class="sd">    order : {None, &quot;C&quot;, &quot;F&quot;, &quot;A&quot;, &quot;K&quot;}, optional</span>
<span class="sd">        Memory layout of the newly output array, if parameter `out` is ``None``.</span>

<span class="sd">        Default: ``&quot;K&quot;``.</span>
<span class="sd">    dtype : {None, str, dtype object}, optional</span>
<span class="sd">        Type to use in computing the vector dot product. By default, the</span>
<span class="sd">        returned array will have data type that is determined by considering</span>
<span class="sd">        Promotion Type Rule and device capabilities.</span>

<span class="sd">        Default: ``None``.</span>
<span class="sd">    axes : {None, list of tuples}, optional</span>
<span class="sd">        A list of tuples with indices of axes the dot product should operate</span>
<span class="sd">        on. For instance, for the signature of ``(i),(i)-&gt;()``, the base</span>
<span class="sd">        elements are vectors and these are taken to be stored in the last axes</span>
<span class="sd">        of each argument. The corresponding axes keyword would be</span>
<span class="sd">        ``[(-1,), (-1,), ()]``.</span>

<span class="sd">        Default: ``None``.</span>
<span class="sd">    axis : {None, int}, optional</span>
<span class="sd">        Axis over which to compute the dot product. This is a short-cut for</span>
<span class="sd">        passing in axes with entries of ``(axis,)`` for each</span>
<span class="sd">        single-core-dimension argument and ``()`` for all others. For instance,</span>
<span class="sd">        for a signature ``(i),(i)-&gt;()``, it is equivalent to passing in</span>
<span class="sd">        ``axes=[(axis,), (axis,), ()]``.</span>

<span class="sd">        Default: ``None``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : dpnp.ndarray</span>
<span class="sd">        The vector dot product of the inputs.</span>
<span class="sd">        This is a 0-d array only when both `x1`, `x2` are 1-d vectors.</span>

<span class="sd">    Limitations</span>
<span class="sd">    -----------</span>
<span class="sd">    Keyword arguments `subok`, and `signature` are only supported with their</span>
<span class="sd">    default values. Otherwise ``NotImplementedError`` exception will be raised.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    :obj:`dpnp.linalg.vecdot` : Array API compatible version.</span>
<span class="sd">    :obj:`dpnp.vdot` : Complex-conjugating dot product but flattens</span>
<span class="sd">                       arguments first.</span>
<span class="sd">    :obj:`dpnp.matmul` : Matrix-matrix product.</span>
<span class="sd">    :obj:`dpnp.vecmat` : Vector-matrix product.</span>
<span class="sd">    :obj:`dpnp.matvec` : Matrix-vector product.</span>
<span class="sd">    :obj:`dpnp.einsum` : Einstein summation convention.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Get the projected size along a given normal for an array of vectors.</span>

<span class="sd">    &gt;&gt;&gt; import dpnp as np</span>
<span class="sd">    &gt;&gt;&gt; v = np.array([[0., 5., 0.], [0., 0., 10.], [0., 6., 8.]])</span>
<span class="sd">    &gt;&gt;&gt; n = np.array([0., 0.6, 0.8])</span>
<span class="sd">    &gt;&gt;&gt; np.vecdot(v, n)</span>
<span class="sd">    array([ 3.,  8., 10.])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dpnp</span><span class="o">.</span><span class="n">check_limitations</span><span class="p">(</span><span class="n">subok_linalg</span><span class="o">=</span><span class="n">subok</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dpnp_vecdot</span><span class="p">(</span>
        <span class="n">x1</span><span class="p">,</span>
        <span class="n">x2</span><span class="p">,</span>
        <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span>
        <span class="n">casting</span><span class="o">=</span><span class="n">casting</span><span class="p">,</span>
        <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span>
        <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
    <span class="p">)</span></div>



<div class="viewcode-block" id="vecmat">
<a class="viewcode-back" href="../../reference/generated/dpnp.vecmat.html#dpnp.vecmat">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">vecmat</span><span class="p">(</span>
    <span class="n">x1</span><span class="p">,</span>
    <span class="n">x2</span><span class="p">,</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">casting</span><span class="o">=</span><span class="s2">&quot;same_kind&quot;</span><span class="p">,</span>
    <span class="n">order</span><span class="o">=</span><span class="s2">&quot;K&quot;</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">subok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">signature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Vector-matrix dot product of two arrays.</span>

<span class="sd">    Given a vector (or stack of vector) :math:`\mathbf{v}` in `x1` and a matrix</span>
<span class="sd">    (or stack of matrices) :math:`\mathbf{A}` in `x2`, the vector-matrix product</span>
<span class="sd">    is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">       \mathbf{v} \cdot \mathbf{A} = \sum_{i=0}^{n-1} \overline{v_i}A_{ij}</span>

<span class="sd">    where the sum is over the last dimension of `x1` and the one-but-last</span>
<span class="sd">    dimensions in `x2` (unless `axes` is specified) and where</span>
<span class="sd">    :math:`\overline{v_i}` denotes the complex conjugate if :math:`{v}` is</span>
<span class="sd">    complex and the identity otherwise. (For a non-conjugated</span>
<span class="sd">    vector-matrix product, use ``dpnp.matvec(x2.mT, x1)``.)</span>

<span class="sd">    For full documentation refer to :obj:`numpy.vecmat`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x1 : {dpnp.ndarray, usm_ndarray}</span>
<span class="sd">        First input array.</span>
<span class="sd">    x2 : {dpnp.ndarray, usm_ndarray}</span>
<span class="sd">        Second input array.</span>
<span class="sd">    out : {None, dpnp.ndarray, usm_ndarray}, optional</span>
<span class="sd">        A location into which the result is stored. If provided, it must have</span>
<span class="sd">        the broadcasted shape of `x1` and `x2` with the summation axis removed.</span>
<span class="sd">        If not provided or ``None``, a freshly-allocated array is used.</span>

<span class="sd">        Default: ``None``.</span>
<span class="sd">    dtype : {None, str, dtype object}, optional</span>
<span class="sd">        Type to use in computing the matrix product. By default, the returned</span>
<span class="sd">        array will have data type that is determined by considering</span>
<span class="sd">        Promotion Type Rule and device capabilities.</span>

<span class="sd">        Default: ``None``.</span>
<span class="sd">    casting : {&quot;no&quot;, &quot;equiv&quot;, &quot;safe&quot;, &quot;same_kind&quot;, &quot;unsafe&quot;}, optional</span>
<span class="sd">        Controls what kind of data casting may occur.</span>

<span class="sd">        Default: ``&quot;same_kind&quot;``.</span>
<span class="sd">    order : {None, &quot;C&quot;, &quot;F&quot;, &quot;A&quot;, &quot;K&quot;}, optional</span>
<span class="sd">        Memory layout of the newly output array, if parameter `out` is ``None``.</span>

<span class="sd">        Default: ``&quot;K&quot;``.</span>
<span class="sd">    axes : {None, list of tuples}, optional</span>
<span class="sd">        A list of tuples with indices of axes the vector-matrix product should</span>
<span class="sd">        operate on. For instance, for the signature of ``(i),(i,j)-&gt;(j)``, the</span>
<span class="sd">        base elements are 1D vectors and 2D matrices, where the vectors are</span>
<span class="sd">        assumed to be stored in the last axis of the first argument, and the</span>
<span class="sd">        matrices in the last two axes of the second argument. The corresponding</span>
<span class="sd">        axes keyword would be ``[(-1,), (-2, -1), (-1,)]``.</span>

<span class="sd">        Default: ``None``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : dpnp.ndarray</span>
<span class="sd">        The vector-matrix product of the inputs.</span>

<span class="sd">    Limitations</span>
<span class="sd">    -----------</span>
<span class="sd">    Keyword arguments `subok`, and `signature`, are only supported with their</span>
<span class="sd">    default values. Otherwise ``NotImplementedError`` exception will be raised.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    :obj:`dpnp.vecdot` : Vector-vector product.</span>
<span class="sd">    :obj:`dpnp.matvec` : Matrix-vector product.</span>
<span class="sd">    :obj:`dpnp.matmul` : Matrix-matrix product.</span>
<span class="sd">    :obj:`dpnp.einsum` : Einstein summation convention.</span>


<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Project a vector along X and Y.</span>

<span class="sd">    &gt;&gt;&gt; import dpnp as np</span>
<span class="sd">    &gt;&gt;&gt; v = np.array([0., 4., 2.])</span>
<span class="sd">    &gt;&gt;&gt; a = np.array([[1., 0., 0.],</span>
<span class="sd">    ...               [0., 1., 0.],</span>
<span class="sd">    ...               [0., 0., 0.]])</span>
<span class="sd">    &gt;&gt;&gt; np.vecmat(v, a)</span>
<span class="sd">    array([0., 4., 0.])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dpnp</span><span class="o">.</span><span class="n">check_limitations</span><span class="p">(</span><span class="n">subok_linalg</span><span class="o">=</span><span class="n">subok</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">)</span>
    <span class="n">dpnp</span><span class="o">.</span><span class="n">check_supported_arrays_type</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>

    <span class="c1"># cannot directly use dpnp.conj(x1) as it returns `int8` for boolean input</span>
    <span class="k">if</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">dpnp</span><span class="o">.</span><span class="n">conj</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dpnp_multiplication</span><span class="p">(</span>
        <span class="s2">&quot;vecmat&quot;</span><span class="p">,</span>
        <span class="n">x1</span><span class="p">,</span>
        <span class="n">x2</span><span class="p">,</span>
        <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span>
        <span class="n">casting</span><span class="o">=</span><span class="n">casting</span><span class="p">,</span>
        <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span>
        <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
    <span class="p">)</span></div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2026, Intel Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>